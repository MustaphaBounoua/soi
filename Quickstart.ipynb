{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.config import get_default_config\n",
    "from src.benchmark.synthetic_dataset import  get_task,get_dataloader\n",
    "from src.libs.soi import  SOI\n",
    "from src.libs.soi_grad import  SOI_grad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Default config\n",
    "args=get_default_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14736860225060955"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a synthetic task with 6 made of two redundant subtasks of 3 variables.\n",
    "my_settings = [{\"rho\":0.6,\"type\":\"red\",\"nb\":3} ]\n",
    "args.dim = 1\n",
    "task_red = get_task(args,my_settings)\n",
    "task_red.o_inf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12391808195228782"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a synthetic task with 6 made of two redundant subtasks of 3 variables.\n",
    "my_settings = [{\"rho\":0.6,\"type\":\"syn\",\"nb\":3} ]\n",
    "args.dim = 1\n",
    "task_syn = get_task(args,my_settings)\n",
    "task_syn.get_summary()[\"o_inf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundancy + Synergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02345052029832928"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make a synthetic task with 6 made of two redundant subtasks of 3 variables.\n",
    "my_settings = [{\"rho\":0.6,\"type\":\"red\",\"nb\":3},{\"rho\":0.6,\"type\":\"syn\",\"nb\":3} ]\n",
    "args.dim = 1\n",
    "task_both = get_task(args,my_settings)\n",
    "task_both.get_summary()[\"o_inf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O-information estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_ema=True\n",
    "args.weight_s_functions = True\n",
    "args.importance_sampling = True\n",
    "args.bs = 512\n",
    "args.warmup_epochs = 1\n",
    "args.max_epochs = 50\n",
    "args.test_epoch = 5\n",
    "args.debug = True\n",
    "args.ou_dir = \"quickstart/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing SOI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the redundancy benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting the scores to learn \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | score     | DiT  | 622 K \n",
      "1 | model_ema | EMA  | 622 K \n",
      "-----------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.980     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521af4e02fb440419d942703dda48d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8960cf708e12491ea416601c0f952c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "`self.log(loss, tensor([1.1350e+00, 1.0654e+00, 1.4112e+00, 2.2466e-01, 6.4033e+00, 1.7580e-01,\n        7.2887e-01, 2.7104e-01, 2.0073e-01, 8.8327e-01, 7.3380e+00, 3.4608e+00,\n        6.9621e+00, 6.1066e+00, 4.1103e-01, 3.9621e-01, 3.9944e+00, 1.3414e+00,\n        4.4706e+00, 2.0739e-01, 4.1320e-01, 7.5555e-01, 2.0520e+00, 2.7108e+00,\n        6.7384e-01, 1.1049e-01, 4.1231e+00, 3.6497e+00, 7.1040e+00, 6.3353e-02,\n        8.8330e-01, 1.2632e+00, 6.2331e-01, 7.9741e-01, 8.8932e-02, 4.4493e-01,\n        1.3486e-01, 3.6341e-02, 3.5191e+00, 1.3659e-02, 4.5157e+00, 5.8442e+00,\n        2.5565e-01, 2.3785e-01, 4.0547e+00, 7.3943e+00, 1.9139e+00, 2.0305e-01,\n        1.2034e+00, 8.1813e+00, 2.2470e+00, 1.4140e+00, 4.3422e-02, 8.4011e+00,\n        3.2438e+00, 3.7961e+00, 3.2243e-03, 7.2467e+00, 2.0582e+00, 1.1413e+01,\n        1.4191e+00, 3.1116e-01, 2.1655e+00, 1.2681e+00, 2.2398e+00, 1.3436e+00,\n        1.0793e+00, 4.5857e+00, 3.0237e+00, 5.6910e+00, 3.4508e+00, 3.8359e+00,\n        1.1552e+01, 1.4283e+00, 8.2866e-02, 4.3769e-03, 2.3756e+00, 2.2200e-01,\n        6.1891e-01, 2.9006e+00, 2.7588e-01, 1.2890e+00, 4.1035e+00, 9.2328e-01,\n        2.0991e-01, 2.0584e-01, 1.4999e+00, 2.8910e-01, 4.3389e+00, 8.3086e+00,\n        6.0969e-01, 7.2746e+00, 5.2187e-01, 5.1438e+00, 2.7500e+00, 6.2608e+00,\n        9.4694e-01, 1.8021e+00, 2.3127e-04, 8.1037e+00, 4.9396e+00, 2.8938e-03,\n        2.1079e+00, 6.1653e-01, 1.5581e-01, 4.7695e+00, 1.4838e+01, 1.7559e+00,\n        3.0995e+00, 2.7602e+00, 5.1479e+00, 9.3750e-02, 1.0245e+01, 1.4201e+00,\n        3.6600e-02, 2.5996e+00, 1.8597e+00, 6.5717e-02, 8.9092e-01, 3.4742e-01,\n        4.9512e+00, 1.7398e+00, 5.9763e+00, 6.1578e-01, 3.2807e-01, 2.4481e+00,\n        2.0668e+00, 3.5684e+00, 4.6043e+00, 9.3965e-01, 1.8551e+00, 4.1542e+00,\n        1.8435e+00, 4.4714e+00, 1.5554e+00, 7.5992e+00, 6.8231e+00, 4.5434e+00,\n        4.7660e+00, 2.0030e-03, 2.4085e+00, 8.6513e+00, 3.8609e+00, 3.4744e+00,\n        9.0022e-01, 5.7450e-01, 3.3041e+00, 9.4426e-01, 1.6875e-01, 1.8315e+00,\n        7.6704e-02, 4.7878e-01, 6.6770e+00, 3.1775e+00, 1.7806e-01, 6.5808e-01,\n        4.3849e-01, 5.1192e-01, 2.2589e+00, 6.1733e+00, 3.5516e-02, 4.3540e-02,\n        9.4210e-01, 9.8835e-01, 7.1511e-03, 7.8297e-03, 7.3587e-01, 1.4759e-02,\n        2.7211e+00, 1.5089e-05, 1.8920e+00, 2.3260e+00, 6.9084e+00, 1.9945e+00,\n        8.8144e-01, 1.8479e+00, 3.7114e+00, 2.0004e-01, 1.2870e+00, 2.4454e+00,\n        4.3721e+00, 1.6549e-02, 3.7370e+00, 7.7004e-01, 3.0222e-01, 6.9388e-01,\n        1.6167e-02, 3.4223e+00, 1.7415e+00, 1.0822e+00, 4.5879e-02, 1.2833e+00,\n        8.0682e-01, 4.5433e+00, 2.5219e+00, 3.1596e+00, 2.0406e+00, 5.7449e+00,\n        4.1129e+00, 1.8592e+00, 2.1841e+00, 1.0141e-01, 4.9456e-01, 1.3056e+01,\n        2.0221e+00, 6.4174e-01, 2.7539e+00, 2.3623e-01, 3.8911e-01, 2.6493e+00,\n        3.7694e+00, 3.5623e-01, 1.2047e+00, 1.1541e+01, 6.7870e+00, 6.9654e-01,\n        2.4637e+00, 4.0743e+00, 3.1263e-01, 4.3306e-01, 3.5404e+00, 4.1814e-01,\n        2.3547e+00, 2.2399e+00, 5.8635e+00, 4.3536e+00, 1.4480e+00, 8.7409e+00,\n        1.5222e+00, 1.3894e-03, 4.3463e+00, 1.5936e+00, 2.7250e+00, 5.5668e+00,\n        1.2400e-01, 2.0132e+00, 1.7924e+00, 5.8413e-03, 2.7054e+00, 9.9026e-04,\n        3.4855e+00, 4.3458e-01, 1.2374e+01, 6.7425e-01, 6.4573e+00, 7.4226e+00,\n        2.6920e+00, 1.9258e+00, 4.8791e-01, 4.4048e+00, 1.1947e+00, 2.3047e-01,\n        1.0788e-02, 9.2372e+00, 1.2185e+01, 2.7588e-01, 6.5412e-02, 1.8451e-01,\n        5.3307e-02, 1.1969e+01, 7.3793e+00, 7.0634e-02, 2.4250e+00, 9.8264e+00,\n        6.3710e+00, 2.1768e+00, 2.0387e+00, 2.7517e+00, 8.0689e+00, 2.2303e-01,\n        3.0499e+00, 4.4130e+00, 6.1101e-02, 1.7545e+00, 5.5046e+00, 3.3874e+00,\n        5.6659e-01, 4.2051e+00, 3.2795e-01, 4.1104e+00, 8.4303e-02, 7.5361e-01,\n        3.9812e-01, 1.2717e+00, 8.8323e+00, 6.5003e+00, 1.1873e+00, 5.8789e+00,\n        6.4000e-01, 4.4284e+00, 6.7618e-01, 9.7483e-01, 4.3808e+00, 5.3069e+00,\n        2.1913e+00, 2.8385e+00, 5.7338e+00, 2.2301e+00, 7.8711e-01, 3.4245e+00,\n        7.6607e-01, 5.1624e-01, 7.1928e+00, 5.4758e+00, 1.0191e+00, 6.6312e+00,\n        4.5566e+00, 4.8646e+00, 3.6004e-01, 9.9408e-01, 3.8161e+00, 9.7279e+00,\n        5.0655e-01, 7.4521e-01, 5.5514e-01, 1.3845e+01, 2.3734e+00, 3.2736e-02,\n        9.8053e-01, 7.9489e+00, 1.6534e+00, 7.0941e-01, 1.0999e-01, 4.0426e-01,\n        4.9481e+00, 8.6310e-02, 4.5669e-01, 1.2970e+01, 1.1514e+00, 8.4022e-01,\n        1.6246e+00, 2.7229e+00, 2.2242e+00, 2.6013e+00, 4.9501e-01, 3.9423e-01,\n        3.4091e-01, 3.4272e+00, 1.1160e+01, 3.9512e-01, 1.1832e+00, 1.2464e-01,\n        9.8178e-02, 1.6613e-01, 4.1290e-01, 6.1244e+00, 1.0231e+00, 1.2503e-01,\n        4.0771e-02, 8.0091e+00, 4.9205e+00, 1.7408e-01, 1.0122e+00, 5.9202e+00,\n        8.1931e-01, 9.6737e-01, 1.9622e+00, 4.5211e-01, 8.2471e+00, 3.1949e-01,\n        2.4146e-05, 1.1555e-01, 1.6288e+00, 9.0632e+00, 5.2839e+00, 2.3084e-01,\n        3.6019e-02, 8.4755e+00, 1.0514e-01, 4.7067e+00, 1.1707e-02, 8.6677e-01,\n        1.2755e-01, 2.2910e+00, 2.5207e+00, 1.3943e+00, 9.6922e+00, 6.4418e-03,\n        2.6435e+00, 1.5587e+00, 1.5745e+00, 6.7337e-02, 3.6581e-01, 1.9353e+00,\n        4.6241e-01, 7.6130e+00, 2.5071e-01, 1.2228e+00, 1.6051e+00, 4.3234e+00,\n        7.4199e-01, 2.6902e+00, 5.4231e+00, 1.2484e-01, 2.9254e+00, 8.4749e-01,\n        2.2955e+00, 1.5539e-02, 1.5911e-04, 2.7150e+00, 2.8486e-01, 6.2073e-01,\n        7.0398e+00, 7.0135e-01, 5.3422e+00, 3.0675e+00, 1.2243e+00, 2.5830e+00,\n        1.9938e+00, 1.3948e-02, 3.3332e+00, 5.7776e+00, 2.6297e-01, 7.4825e-01,\n        1.0251e-03, 1.0122e+00, 3.4252e-01, 2.3412e+00, 1.9059e+00, 8.1557e-02,\n        3.6914e-01, 1.4041e+01, 1.1974e-01, 1.0951e+00, 2.1961e-01, 6.2026e-02,\n        8.9509e-02, 3.2480e+00, 4.6860e-01, 1.5804e+00, 5.4547e-01, 9.0526e-01,\n        7.8704e-01, 4.9906e-01, 4.3199e-01, 8.9556e-01, 1.1273e+00, 8.8593e-01,\n        2.7353e+00, 2.3644e+00, 2.0056e-01, 1.2990e+00, 2.7394e+00, 4.0923e+00,\n        5.2077e-01, 5.5011e-02, 8.4863e-01, 3.8805e+00, 5.3181e-02, 1.4201e-01,\n        9.0159e-01, 2.0430e+00, 1.0488e+00, 3.0121e-01, 1.5229e-03, 5.8190e+00,\n        9.7950e+00, 2.2547e-01, 1.4066e+00, 6.5591e-01, 3.9774e+00, 2.9609e+00,\n        5.7000e-01, 4.4174e-01, 5.4463e-04, 1.4024e-01, 5.9563e+00, 5.1071e+00,\n        2.6194e-01, 1.2713e+01, 1.6933e+00, 3.5996e+00, 5.3956e+00, 3.8350e+00,\n        3.4070e+00, 3.7182e+00, 5.5391e-03, 4.3986e-01, 5.1933e-01, 1.2569e+00,\n        1.9752e+00, 1.3771e+00, 3.9889e-01, 4.6623e+00, 2.3725e+00, 3.6287e+00,\n        6.5457e-01, 1.1658e+00, 5.2813e-01, 1.5082e+00, 4.1374e+00, 2.2196e+00,\n        1.8648e+00, 7.9024e-02, 1.8012e-01, 6.4865e-01, 4.3497e-01, 1.9303e+00,\n        4.7323e-01, 3.0726e-02, 3.4314e+00, 1.1443e+00, 4.7556e+00, 5.9895e+00,\n        4.4451e-01, 1.2577e-01, 3.2947e+00, 6.4019e-01, 3.7777e+00, 2.6863e-02,\n        8.8693e-01, 5.8376e-02], device='cuda:0', dtype=torch.float64))` was called, but the tensor must have a single element. You can try doing `self.log(loss, tensor([1.1350e+00, 1.0654e+00, 1.4112e+00, 2.2466e-01, 6.4033e+00, 1.7580e-01,\n        7.2887e-01, 2.7104e-01, 2.0073e-01, 8.8327e-01, 7.3380e+00, 3.4608e+00,\n        6.9621e+00, 6.1066e+00, 4.1103e-01, 3.9621e-01, 3.9944e+00, 1.3414e+00,\n        4.4706e+00, 2.0739e-01, 4.1320e-01, 7.5555e-01, 2.0520e+00, 2.7108e+00,\n        6.7384e-01, 1.1049e-01, 4.1231e+00, 3.6497e+00, 7.1040e+00, 6.3353e-02,\n        8.8330e-01, 1.2632e+00, 6.2331e-01, 7.9741e-01, 8.8932e-02, 4.4493e-01,\n        1.3486e-01, 3.6341e-02, 3.5191e+00, 1.3659e-02, 4.5157e+00, 5.8442e+00,\n        2.5565e-01, 2.3785e-01, 4.0547e+00, 7.3943e+00, 1.9139e+00, 2.0305e-01,\n        1.2034e+00, 8.1813e+00, 2.2470e+00, 1.4140e+00, 4.3422e-02, 8.4011e+00,\n        3.2438e+00, 3.7961e+00, 3.2243e-03, 7.2467e+00, 2.0582e+00, 1.1413e+01,\n        1.4191e+00, 3.1116e-01, 2.1655e+00, 1.2681e+00, 2.2398e+00, 1.3436e+00,\n        1.0793e+00, 4.5857e+00, 3.0237e+00, 5.6910e+00, 3.4508e+00, 3.8359e+00,\n        1.1552e+01, 1.4283e+00, 8.2866e-02, 4.3769e-03, 2.3756e+00, 2.2200e-01,\n        6.1891e-01, 2.9006e+00, 2.7588e-01, 1.2890e+00, 4.1035e+00, 9.2328e-01,\n        2.0991e-01, 2.0584e-01, 1.4999e+00, 2.8910e-01, 4.3389e+00, 8.3086e+00,\n        6.0969e-01, 7.2746e+00, 5.2187e-01, 5.1438e+00, 2.7500e+00, 6.2608e+00,\n        9.4694e-01, 1.8021e+00, 2.3127e-04, 8.1037e+00, 4.9396e+00, 2.8938e-03,\n        2.1079e+00, 6.1653e-01, 1.5581e-01, 4.7695e+00, 1.4838e+01, 1.7559e+00,\n        3.0995e+00, 2.7602e+00, 5.1479e+00, 9.3750e-02, 1.0245e+01, 1.4201e+00,\n        3.6600e-02, 2.5996e+00, 1.8597e+00, 6.5717e-02, 8.9092e-01, 3.4742e-01,\n        4.9512e+00, 1.7398e+00, 5.9763e+00, 6.1578e-01, 3.2807e-01, 2.4481e+00,\n        2.0668e+00, 3.5684e+00, 4.6043e+00, 9.3965e-01, 1.8551e+00, 4.1542e+00,\n        1.8435e+00, 4.4714e+00, 1.5554e+00, 7.5992e+00, 6.8231e+00, 4.5434e+00,\n        4.7660e+00, 2.0030e-03, 2.4085e+00, 8.6513e+00, 3.8609e+00, 3.4744e+00,\n        9.0022e-01, 5.7450e-01, 3.3041e+00, 9.4426e-01, 1.6875e-01, 1.8315e+00,\n        7.6704e-02, 4.7878e-01, 6.6770e+00, 3.1775e+00, 1.7806e-01, 6.5808e-01,\n        4.3849e-01, 5.1192e-01, 2.2589e+00, 6.1733e+00, 3.5516e-02, 4.3540e-02,\n        9.4210e-01, 9.8835e-01, 7.1511e-03, 7.8297e-03, 7.3587e-01, 1.4759e-02,\n        2.7211e+00, 1.5089e-05, 1.8920e+00, 2.3260e+00, 6.9084e+00, 1.9945e+00,\n        8.8144e-01, 1.8479e+00, 3.7114e+00, 2.0004e-01, 1.2870e+00, 2.4454e+00,\n        4.3721e+00, 1.6549e-02, 3.7370e+00, 7.7004e-01, 3.0222e-01, 6.9388e-01,\n        1.6167e-02, 3.4223e+00, 1.7415e+00, 1.0822e+00, 4.5879e-02, 1.2833e+00,\n        8.0682e-01, 4.5433e+00, 2.5219e+00, 3.1596e+00, 2.0406e+00, 5.7449e+00,\n        4.1129e+00, 1.8592e+00, 2.1841e+00, 1.0141e-01, 4.9456e-01, 1.3056e+01,\n        2.0221e+00, 6.4174e-01, 2.7539e+00, 2.3623e-01, 3.8911e-01, 2.6493e+00,\n        3.7694e+00, 3.5623e-01, 1.2047e+00, 1.1541e+01, 6.7870e+00, 6.9654e-01,\n        2.4637e+00, 4.0743e+00, 3.1263e-01, 4.3306e-01, 3.5404e+00, 4.1814e-01,\n        2.3547e+00, 2.2399e+00, 5.8635e+00, 4.3536e+00, 1.4480e+00, 8.7409e+00,\n        1.5222e+00, 1.3894e-03, 4.3463e+00, 1.5936e+00, 2.7250e+00, 5.5668e+00,\n        1.2400e-01, 2.0132e+00, 1.7924e+00, 5.8413e-03, 2.7054e+00, 9.9026e-04,\n        3.4855e+00, 4.3458e-01, 1.2374e+01, 6.7425e-01, 6.4573e+00, 7.4226e+00,\n        2.6920e+00, 1.9258e+00, 4.8791e-01, 4.4048e+00, 1.1947e+00, 2.3047e-01,\n        1.0788e-02, 9.2372e+00, 1.2185e+01, 2.7588e-01, 6.5412e-02, 1.8451e-01,\n        5.3307e-02, 1.1969e+01, 7.3793e+00, 7.0634e-02, 2.4250e+00, 9.8264e+00,\n        6.3710e+00, 2.1768e+00, 2.0387e+00, 2.7517e+00, 8.0689e+00, 2.2303e-01,\n        3.0499e+00, 4.4130e+00, 6.1101e-02, 1.7545e+00, 5.5046e+00, 3.3874e+00,\n        5.6659e-01, 4.2051e+00, 3.2795e-01, 4.1104e+00, 8.4303e-02, 7.5361e-01,\n        3.9812e-01, 1.2717e+00, 8.8323e+00, 6.5003e+00, 1.1873e+00, 5.8789e+00,\n        6.4000e-01, 4.4284e+00, 6.7618e-01, 9.7483e-01, 4.3808e+00, 5.3069e+00,\n        2.1913e+00, 2.8385e+00, 5.7338e+00, 2.2301e+00, 7.8711e-01, 3.4245e+00,\n        7.6607e-01, 5.1624e-01, 7.1928e+00, 5.4758e+00, 1.0191e+00, 6.6312e+00,\n        4.5566e+00, 4.8646e+00, 3.6004e-01, 9.9408e-01, 3.8161e+00, 9.7279e+00,\n        5.0655e-01, 7.4521e-01, 5.5514e-01, 1.3845e+01, 2.3734e+00, 3.2736e-02,\n        9.8053e-01, 7.9489e+00, 1.6534e+00, 7.0941e-01, 1.0999e-01, 4.0426e-01,\n        4.9481e+00, 8.6310e-02, 4.5669e-01, 1.2970e+01, 1.1514e+00, 8.4022e-01,\n        1.6246e+00, 2.7229e+00, 2.2242e+00, 2.6013e+00, 4.9501e-01, 3.9423e-01,\n        3.4091e-01, 3.4272e+00, 1.1160e+01, 3.9512e-01, 1.1832e+00, 1.2464e-01,\n        9.8178e-02, 1.6613e-01, 4.1290e-01, 6.1244e+00, 1.0231e+00, 1.2503e-01,\n        4.0771e-02, 8.0091e+00, 4.9205e+00, 1.7408e-01, 1.0122e+00, 5.9202e+00,\n        8.1931e-01, 9.6737e-01, 1.9622e+00, 4.5211e-01, 8.2471e+00, 3.1949e-01,\n        2.4146e-05, 1.1555e-01, 1.6288e+00, 9.0632e+00, 5.2839e+00, 2.3084e-01,\n        3.6019e-02, 8.4755e+00, 1.0514e-01, 4.7067e+00, 1.1707e-02, 8.6677e-01,\n        1.2755e-01, 2.2910e+00, 2.5207e+00, 1.3943e+00, 9.6922e+00, 6.4418e-03,\n        2.6435e+00, 1.5587e+00, 1.5745e+00, 6.7337e-02, 3.6581e-01, 1.9353e+00,\n        4.6241e-01, 7.6130e+00, 2.5071e-01, 1.2228e+00, 1.6051e+00, 4.3234e+00,\n        7.4199e-01, 2.6902e+00, 5.4231e+00, 1.2484e-01, 2.9254e+00, 8.4749e-01,\n        2.2955e+00, 1.5539e-02, 1.5911e-04, 2.7150e+00, 2.8486e-01, 6.2073e-01,\n        7.0398e+00, 7.0135e-01, 5.3422e+00, 3.0675e+00, 1.2243e+00, 2.5830e+00,\n        1.9938e+00, 1.3948e-02, 3.3332e+00, 5.7776e+00, 2.6297e-01, 7.4825e-01,\n        1.0251e-03, 1.0122e+00, 3.4252e-01, 2.3412e+00, 1.9059e+00, 8.1557e-02,\n        3.6914e-01, 1.4041e+01, 1.1974e-01, 1.0951e+00, 2.1961e-01, 6.2026e-02,\n        8.9509e-02, 3.2480e+00, 4.6860e-01, 1.5804e+00, 5.4547e-01, 9.0526e-01,\n        7.8704e-01, 4.9906e-01, 4.3199e-01, 8.9556e-01, 1.1273e+00, 8.8593e-01,\n        2.7353e+00, 2.3644e+00, 2.0056e-01, 1.2990e+00, 2.7394e+00, 4.0923e+00,\n        5.2077e-01, 5.5011e-02, 8.4863e-01, 3.8805e+00, 5.3181e-02, 1.4201e-01,\n        9.0159e-01, 2.0430e+00, 1.0488e+00, 3.0121e-01, 1.5229e-03, 5.8190e+00,\n        9.7950e+00, 2.2547e-01, 1.4066e+00, 6.5591e-01, 3.9774e+00, 2.9609e+00,\n        5.7000e-01, 4.4174e-01, 5.4463e-04, 1.4024e-01, 5.9563e+00, 5.1071e+00,\n        2.6194e-01, 1.2713e+01, 1.6933e+00, 3.5996e+00, 5.3956e+00, 3.8350e+00,\n        3.4070e+00, 3.7182e+00, 5.5391e-03, 4.3986e-01, 5.1933e-01, 1.2569e+00,\n        1.9752e+00, 1.3771e+00, 3.9889e-01, 4.6623e+00, 2.3725e+00, 3.6287e+00,\n        6.5457e-01, 1.1658e+00, 5.2813e-01, 1.5082e+00, 4.1374e+00, 2.2196e+00,\n        1.8648e+00, 7.9024e-02, 1.8012e-01, 6.4865e-01, 4.3497e-01, 1.9303e+00,\n        4.7323e-01, 3.0726e-02, 3.4314e+00, 1.1443e+00, 4.7556e+00, 5.9895e+00,\n        4.4451e-01, 1.2577e-01, 3.2947e+00, 6.4019e-01, 3.7777e+00, 2.6863e-02,\n        8.8693e-01, 5.8376e-02], device='cuda:0', dtype=torch.float64).mean())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m soi \u001b[38;5;241m=\u001b[39m SOI(args, nb_var \u001b[38;5;241m=\u001b[39m task_red\u001b[38;5;241m.\u001b[39mnb_var, gt \u001b[38;5;241m=\u001b[39m gt)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## Fit the model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43msoi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m## Compute O_information using the test_loader\u001b[39;00m\n\u001b[1;32m      8\u001b[0m results \u001b[38;5;241m=\u001b[39m soi\u001b[38;5;241m.\u001b[39mcompute_o_inf()\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/src/libs/soi.py:69\u001b[0m, in \u001b[0;36mSOI.fit\u001b[0;34m(self, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     59\u001b[0m CHECKPOINT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/Soi\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/seed_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/setting_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/dim_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/rho_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mout_dir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mo_inf_order\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m                                                                            args\u001b[38;5;241m.\u001b[39mbenchmark,\n\u001b[1;32m     61\u001b[0m args\u001b[38;5;241m.\u001b[39mtransformation, args\u001b[38;5;241m.\u001b[39mseed, args\u001b[38;5;241m.\u001b[39msetting, args\u001b[38;5;241m.\u001b[39mdim, args\u001b[38;5;241m.\u001b[39mrho)\n\u001b[1;32m     62\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(logger\u001b[38;5;241m=\u001b[39mpl\u001b[38;5;241m.\u001b[39mloggers\u001b[38;5;241m.\u001b[39mTensorBoardLogger(save_dir\u001b[38;5;241m=\u001b[39mCHECKPOINT_DIR, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     63\u001b[0m                  default_root_dir\u001b[38;5;241m=\u001b[39mCHECKPOINT_DIR,\n\u001b[1;32m     64\u001b[0m                  accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maccelerator,\n\u001b[1;32m     65\u001b[0m                  devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevices,\n\u001b[1;32m     66\u001b[0m                  max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_epochs,\n\u001b[1;32m     67\u001b[0m                  check_val_every_n_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mcheck_val_every_n_epoch)  \n\u001b[0;32m---> 69\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         closure()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/core/module.py:1303\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:152\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/torch/optim/adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 146\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    149\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     97\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     99\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/automatic.py:318\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/src/libs/soi.py:79\u001b[0m, in \u001b[0;36mSOI.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     78\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msde\u001b[38;5;241m.\u001b[39mtrain_step(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_forward)\u001b[38;5;66;03m#.mean()\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/core/module.py:470\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dataloader_idx_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou called `self.log` with the key `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it should not contain information about `dataloader_idx`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     )\n\u001b[0;32m--> 470\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumbers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNumber\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__to_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mshould_reset_tensors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_fx_name):\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# if we started a new epoch (running its first batch) the hook name has changed\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# reset any tensors for the new hook name\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     results\u001b[38;5;241m.\u001b[39mreset(metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_fx_name)\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/lightning_utilities/core/apply_func.py:64\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# fast path for the most common cases:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [function(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m/home/bounoua/work/soi_clean/soi/myvenv/lib/python3.9/site-packages/pytorch_lightning/core/module.py:649\u001b[0m, in \u001b[0;36mLightningModule.__to_tensor\u001b[0;34m(self, value, name)\u001b[0m\n\u001b[1;32m    643\u001b[0m value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    644\u001b[0m     value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor)\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39m_get_default_dtype())\n\u001b[1;32m    647\u001b[0m )\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnumel(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    650\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)` was called, but the tensor must have a single element.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can try doing `self.log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mean())`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    653\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[0;31mValueError\u001b[0m: `self.log(loss, tensor([1.1350e+00, 1.0654e+00, 1.4112e+00, 2.2466e-01, 6.4033e+00, 1.7580e-01,\n        7.2887e-01, 2.7104e-01, 2.0073e-01, 8.8327e-01, 7.3380e+00, 3.4608e+00,\n        6.9621e+00, 6.1066e+00, 4.1103e-01, 3.9621e-01, 3.9944e+00, 1.3414e+00,\n        4.4706e+00, 2.0739e-01, 4.1320e-01, 7.5555e-01, 2.0520e+00, 2.7108e+00,\n        6.7384e-01, 1.1049e-01, 4.1231e+00, 3.6497e+00, 7.1040e+00, 6.3353e-02,\n        8.8330e-01, 1.2632e+00, 6.2331e-01, 7.9741e-01, 8.8932e-02, 4.4493e-01,\n        1.3486e-01, 3.6341e-02, 3.5191e+00, 1.3659e-02, 4.5157e+00, 5.8442e+00,\n        2.5565e-01, 2.3785e-01, 4.0547e+00, 7.3943e+00, 1.9139e+00, 2.0305e-01,\n        1.2034e+00, 8.1813e+00, 2.2470e+00, 1.4140e+00, 4.3422e-02, 8.4011e+00,\n        3.2438e+00, 3.7961e+00, 3.2243e-03, 7.2467e+00, 2.0582e+00, 1.1413e+01,\n        1.4191e+00, 3.1116e-01, 2.1655e+00, 1.2681e+00, 2.2398e+00, 1.3436e+00,\n        1.0793e+00, 4.5857e+00, 3.0237e+00, 5.6910e+00, 3.4508e+00, 3.8359e+00,\n        1.1552e+01, 1.4283e+00, 8.2866e-02, 4.3769e-03, 2.3756e+00, 2.2200e-01,\n        6.1891e-01, 2.9006e+00, 2.7588e-01, 1.2890e+00, 4.1035e+00, 9.2328e-01,\n        2.0991e-01, 2.0584e-01, 1.4999e+00, 2.8910e-01, 4.3389e+00, 8.3086e+00,\n        6.0969e-01, 7.2746e+00, 5.2187e-01, 5.1438e+00, 2.7500e+00, 6.2608e+00,\n        9.4694e-01, 1.8021e+00, 2.3127e-04, 8.1037e+00, 4.9396e+00, 2.8938e-03,\n        2.1079e+00, 6.1653e-01, 1.5581e-01, 4.7695e+00, 1.4838e+01, 1.7559e+00,\n        3.0995e+00, 2.7602e+00, 5.1479e+00, 9.3750e-02, 1.0245e+01, 1.4201e+00,\n        3.6600e-02, 2.5996e+00, 1.8597e+00, 6.5717e-02, 8.9092e-01, 3.4742e-01,\n        4.9512e+00, 1.7398e+00, 5.9763e+00, 6.1578e-01, 3.2807e-01, 2.4481e+00,\n        2.0668e+00, 3.5684e+00, 4.6043e+00, 9.3965e-01, 1.8551e+00, 4.1542e+00,\n        1.8435e+00, 4.4714e+00, 1.5554e+00, 7.5992e+00, 6.8231e+00, 4.5434e+00,\n        4.7660e+00, 2.0030e-03, 2.4085e+00, 8.6513e+00, 3.8609e+00, 3.4744e+00,\n        9.0022e-01, 5.7450e-01, 3.3041e+00, 9.4426e-01, 1.6875e-01, 1.8315e+00,\n        7.6704e-02, 4.7878e-01, 6.6770e+00, 3.1775e+00, 1.7806e-01, 6.5808e-01,\n        4.3849e-01, 5.1192e-01, 2.2589e+00, 6.1733e+00, 3.5516e-02, 4.3540e-02,\n        9.4210e-01, 9.8835e-01, 7.1511e-03, 7.8297e-03, 7.3587e-01, 1.4759e-02,\n        2.7211e+00, 1.5089e-05, 1.8920e+00, 2.3260e+00, 6.9084e+00, 1.9945e+00,\n        8.8144e-01, 1.8479e+00, 3.7114e+00, 2.0004e-01, 1.2870e+00, 2.4454e+00,\n        4.3721e+00, 1.6549e-02, 3.7370e+00, 7.7004e-01, 3.0222e-01, 6.9388e-01,\n        1.6167e-02, 3.4223e+00, 1.7415e+00, 1.0822e+00, 4.5879e-02, 1.2833e+00,\n        8.0682e-01, 4.5433e+00, 2.5219e+00, 3.1596e+00, 2.0406e+00, 5.7449e+00,\n        4.1129e+00, 1.8592e+00, 2.1841e+00, 1.0141e-01, 4.9456e-01, 1.3056e+01,\n        2.0221e+00, 6.4174e-01, 2.7539e+00, 2.3623e-01, 3.8911e-01, 2.6493e+00,\n        3.7694e+00, 3.5623e-01, 1.2047e+00, 1.1541e+01, 6.7870e+00, 6.9654e-01,\n        2.4637e+00, 4.0743e+00, 3.1263e-01, 4.3306e-01, 3.5404e+00, 4.1814e-01,\n        2.3547e+00, 2.2399e+00, 5.8635e+00, 4.3536e+00, 1.4480e+00, 8.7409e+00,\n        1.5222e+00, 1.3894e-03, 4.3463e+00, 1.5936e+00, 2.7250e+00, 5.5668e+00,\n        1.2400e-01, 2.0132e+00, 1.7924e+00, 5.8413e-03, 2.7054e+00, 9.9026e-04,\n        3.4855e+00, 4.3458e-01, 1.2374e+01, 6.7425e-01, 6.4573e+00, 7.4226e+00,\n        2.6920e+00, 1.9258e+00, 4.8791e-01, 4.4048e+00, 1.1947e+00, 2.3047e-01,\n        1.0788e-02, 9.2372e+00, 1.2185e+01, 2.7588e-01, 6.5412e-02, 1.8451e-01,\n        5.3307e-02, 1.1969e+01, 7.3793e+00, 7.0634e-02, 2.4250e+00, 9.8264e+00,\n        6.3710e+00, 2.1768e+00, 2.0387e+00, 2.7517e+00, 8.0689e+00, 2.2303e-01,\n        3.0499e+00, 4.4130e+00, 6.1101e-02, 1.7545e+00, 5.5046e+00, 3.3874e+00,\n        5.6659e-01, 4.2051e+00, 3.2795e-01, 4.1104e+00, 8.4303e-02, 7.5361e-01,\n        3.9812e-01, 1.2717e+00, 8.8323e+00, 6.5003e+00, 1.1873e+00, 5.8789e+00,\n        6.4000e-01, 4.4284e+00, 6.7618e-01, 9.7483e-01, 4.3808e+00, 5.3069e+00,\n        2.1913e+00, 2.8385e+00, 5.7338e+00, 2.2301e+00, 7.8711e-01, 3.4245e+00,\n        7.6607e-01, 5.1624e-01, 7.1928e+00, 5.4758e+00, 1.0191e+00, 6.6312e+00,\n        4.5566e+00, 4.8646e+00, 3.6004e-01, 9.9408e-01, 3.8161e+00, 9.7279e+00,\n        5.0655e-01, 7.4521e-01, 5.5514e-01, 1.3845e+01, 2.3734e+00, 3.2736e-02,\n        9.8053e-01, 7.9489e+00, 1.6534e+00, 7.0941e-01, 1.0999e-01, 4.0426e-01,\n        4.9481e+00, 8.6310e-02, 4.5669e-01, 1.2970e+01, 1.1514e+00, 8.4022e-01,\n        1.6246e+00, 2.7229e+00, 2.2242e+00, 2.6013e+00, 4.9501e-01, 3.9423e-01,\n        3.4091e-01, 3.4272e+00, 1.1160e+01, 3.9512e-01, 1.1832e+00, 1.2464e-01,\n        9.8178e-02, 1.6613e-01, 4.1290e-01, 6.1244e+00, 1.0231e+00, 1.2503e-01,\n        4.0771e-02, 8.0091e+00, 4.9205e+00, 1.7408e-01, 1.0122e+00, 5.9202e+00,\n        8.1931e-01, 9.6737e-01, 1.9622e+00, 4.5211e-01, 8.2471e+00, 3.1949e-01,\n        2.4146e-05, 1.1555e-01, 1.6288e+00, 9.0632e+00, 5.2839e+00, 2.3084e-01,\n        3.6019e-02, 8.4755e+00, 1.0514e-01, 4.7067e+00, 1.1707e-02, 8.6677e-01,\n        1.2755e-01, 2.2910e+00, 2.5207e+00, 1.3943e+00, 9.6922e+00, 6.4418e-03,\n        2.6435e+00, 1.5587e+00, 1.5745e+00, 6.7337e-02, 3.6581e-01, 1.9353e+00,\n        4.6241e-01, 7.6130e+00, 2.5071e-01, 1.2228e+00, 1.6051e+00, 4.3234e+00,\n        7.4199e-01, 2.6902e+00, 5.4231e+00, 1.2484e-01, 2.9254e+00, 8.4749e-01,\n        2.2955e+00, 1.5539e-02, 1.5911e-04, 2.7150e+00, 2.8486e-01, 6.2073e-01,\n        7.0398e+00, 7.0135e-01, 5.3422e+00, 3.0675e+00, 1.2243e+00, 2.5830e+00,\n        1.9938e+00, 1.3948e-02, 3.3332e+00, 5.7776e+00, 2.6297e-01, 7.4825e-01,\n        1.0251e-03, 1.0122e+00, 3.4252e-01, 2.3412e+00, 1.9059e+00, 8.1557e-02,\n        3.6914e-01, 1.4041e+01, 1.1974e-01, 1.0951e+00, 2.1961e-01, 6.2026e-02,\n        8.9509e-02, 3.2480e+00, 4.6860e-01, 1.5804e+00, 5.4547e-01, 9.0526e-01,\n        7.8704e-01, 4.9906e-01, 4.3199e-01, 8.9556e-01, 1.1273e+00, 8.8593e-01,\n        2.7353e+00, 2.3644e+00, 2.0056e-01, 1.2990e+00, 2.7394e+00, 4.0923e+00,\n        5.2077e-01, 5.5011e-02, 8.4863e-01, 3.8805e+00, 5.3181e-02, 1.4201e-01,\n        9.0159e-01, 2.0430e+00, 1.0488e+00, 3.0121e-01, 1.5229e-03, 5.8190e+00,\n        9.7950e+00, 2.2547e-01, 1.4066e+00, 6.5591e-01, 3.9774e+00, 2.9609e+00,\n        5.7000e-01, 4.4174e-01, 5.4463e-04, 1.4024e-01, 5.9563e+00, 5.1071e+00,\n        2.6194e-01, 1.2713e+01, 1.6933e+00, 3.5996e+00, 5.3956e+00, 3.8350e+00,\n        3.4070e+00, 3.7182e+00, 5.5391e-03, 4.3986e-01, 5.1933e-01, 1.2569e+00,\n        1.9752e+00, 1.3771e+00, 3.9889e-01, 4.6623e+00, 2.3725e+00, 3.6287e+00,\n        6.5457e-01, 1.1658e+00, 5.2813e-01, 1.5082e+00, 4.1374e+00, 2.2196e+00,\n        1.8648e+00, 7.9024e-02, 1.8012e-01, 6.4865e-01, 4.3497e-01, 1.9303e+00,\n        4.7323e-01, 3.0726e-02, 3.4314e+00, 1.1443e+00, 4.7556e+00, 5.9895e+00,\n        4.4451e-01, 1.2577e-01, 3.2947e+00, 6.4019e-01, 3.7777e+00, 2.6863e-02,\n        8.8693e-01, 5.8376e-02], device='cuda:0', dtype=torch.float64))` was called, but the tensor must have a single element. You can try doing `self.log(loss, tensor([1.1350e+00, 1.0654e+00, 1.4112e+00, 2.2466e-01, 6.4033e+00, 1.7580e-01,\n        7.2887e-01, 2.7104e-01, 2.0073e-01, 8.8327e-01, 7.3380e+00, 3.4608e+00,\n        6.9621e+00, 6.1066e+00, 4.1103e-01, 3.9621e-01, 3.9944e+00, 1.3414e+00,\n        4.4706e+00, 2.0739e-01, 4.1320e-01, 7.5555e-01, 2.0520e+00, 2.7108e+00,\n        6.7384e-01, 1.1049e-01, 4.1231e+00, 3.6497e+00, 7.1040e+00, 6.3353e-02,\n        8.8330e-01, 1.2632e+00, 6.2331e-01, 7.9741e-01, 8.8932e-02, 4.4493e-01,\n        1.3486e-01, 3.6341e-02, 3.5191e+00, 1.3659e-02, 4.5157e+00, 5.8442e+00,\n        2.5565e-01, 2.3785e-01, 4.0547e+00, 7.3943e+00, 1.9139e+00, 2.0305e-01,\n        1.2034e+00, 8.1813e+00, 2.2470e+00, 1.4140e+00, 4.3422e-02, 8.4011e+00,\n        3.2438e+00, 3.7961e+00, 3.2243e-03, 7.2467e+00, 2.0582e+00, 1.1413e+01,\n        1.4191e+00, 3.1116e-01, 2.1655e+00, 1.2681e+00, 2.2398e+00, 1.3436e+00,\n        1.0793e+00, 4.5857e+00, 3.0237e+00, 5.6910e+00, 3.4508e+00, 3.8359e+00,\n        1.1552e+01, 1.4283e+00, 8.2866e-02, 4.3769e-03, 2.3756e+00, 2.2200e-01,\n        6.1891e-01, 2.9006e+00, 2.7588e-01, 1.2890e+00, 4.1035e+00, 9.2328e-01,\n        2.0991e-01, 2.0584e-01, 1.4999e+00, 2.8910e-01, 4.3389e+00, 8.3086e+00,\n        6.0969e-01, 7.2746e+00, 5.2187e-01, 5.1438e+00, 2.7500e+00, 6.2608e+00,\n        9.4694e-01, 1.8021e+00, 2.3127e-04, 8.1037e+00, 4.9396e+00, 2.8938e-03,\n        2.1079e+00, 6.1653e-01, 1.5581e-01, 4.7695e+00, 1.4838e+01, 1.7559e+00,\n        3.0995e+00, 2.7602e+00, 5.1479e+00, 9.3750e-02, 1.0245e+01, 1.4201e+00,\n        3.6600e-02, 2.5996e+00, 1.8597e+00, 6.5717e-02, 8.9092e-01, 3.4742e-01,\n        4.9512e+00, 1.7398e+00, 5.9763e+00, 6.1578e-01, 3.2807e-01, 2.4481e+00,\n        2.0668e+00, 3.5684e+00, 4.6043e+00, 9.3965e-01, 1.8551e+00, 4.1542e+00,\n        1.8435e+00, 4.4714e+00, 1.5554e+00, 7.5992e+00, 6.8231e+00, 4.5434e+00,\n        4.7660e+00, 2.0030e-03, 2.4085e+00, 8.6513e+00, 3.8609e+00, 3.4744e+00,\n        9.0022e-01, 5.7450e-01, 3.3041e+00, 9.4426e-01, 1.6875e-01, 1.8315e+00,\n        7.6704e-02, 4.7878e-01, 6.6770e+00, 3.1775e+00, 1.7806e-01, 6.5808e-01,\n        4.3849e-01, 5.1192e-01, 2.2589e+00, 6.1733e+00, 3.5516e-02, 4.3540e-02,\n        9.4210e-01, 9.8835e-01, 7.1511e-03, 7.8297e-03, 7.3587e-01, 1.4759e-02,\n        2.7211e+00, 1.5089e-05, 1.8920e+00, 2.3260e+00, 6.9084e+00, 1.9945e+00,\n        8.8144e-01, 1.8479e+00, 3.7114e+00, 2.0004e-01, 1.2870e+00, 2.4454e+00,\n        4.3721e+00, 1.6549e-02, 3.7370e+00, 7.7004e-01, 3.0222e-01, 6.9388e-01,\n        1.6167e-02, 3.4223e+00, 1.7415e+00, 1.0822e+00, 4.5879e-02, 1.2833e+00,\n        8.0682e-01, 4.5433e+00, 2.5219e+00, 3.1596e+00, 2.0406e+00, 5.7449e+00,\n        4.1129e+00, 1.8592e+00, 2.1841e+00, 1.0141e-01, 4.9456e-01, 1.3056e+01,\n        2.0221e+00, 6.4174e-01, 2.7539e+00, 2.3623e-01, 3.8911e-01, 2.6493e+00,\n        3.7694e+00, 3.5623e-01, 1.2047e+00, 1.1541e+01, 6.7870e+00, 6.9654e-01,\n        2.4637e+00, 4.0743e+00, 3.1263e-01, 4.3306e-01, 3.5404e+00, 4.1814e-01,\n        2.3547e+00, 2.2399e+00, 5.8635e+00, 4.3536e+00, 1.4480e+00, 8.7409e+00,\n        1.5222e+00, 1.3894e-03, 4.3463e+00, 1.5936e+00, 2.7250e+00, 5.5668e+00,\n        1.2400e-01, 2.0132e+00, 1.7924e+00, 5.8413e-03, 2.7054e+00, 9.9026e-04,\n        3.4855e+00, 4.3458e-01, 1.2374e+01, 6.7425e-01, 6.4573e+00, 7.4226e+00,\n        2.6920e+00, 1.9258e+00, 4.8791e-01, 4.4048e+00, 1.1947e+00, 2.3047e-01,\n        1.0788e-02, 9.2372e+00, 1.2185e+01, 2.7588e-01, 6.5412e-02, 1.8451e-01,\n        5.3307e-02, 1.1969e+01, 7.3793e+00, 7.0634e-02, 2.4250e+00, 9.8264e+00,\n        6.3710e+00, 2.1768e+00, 2.0387e+00, 2.7517e+00, 8.0689e+00, 2.2303e-01,\n        3.0499e+00, 4.4130e+00, 6.1101e-02, 1.7545e+00, 5.5046e+00, 3.3874e+00,\n        5.6659e-01, 4.2051e+00, 3.2795e-01, 4.1104e+00, 8.4303e-02, 7.5361e-01,\n        3.9812e-01, 1.2717e+00, 8.8323e+00, 6.5003e+00, 1.1873e+00, 5.8789e+00,\n        6.4000e-01, 4.4284e+00, 6.7618e-01, 9.7483e-01, 4.3808e+00, 5.3069e+00,\n        2.1913e+00, 2.8385e+00, 5.7338e+00, 2.2301e+00, 7.8711e-01, 3.4245e+00,\n        7.6607e-01, 5.1624e-01, 7.1928e+00, 5.4758e+00, 1.0191e+00, 6.6312e+00,\n        4.5566e+00, 4.8646e+00, 3.6004e-01, 9.9408e-01, 3.8161e+00, 9.7279e+00,\n        5.0655e-01, 7.4521e-01, 5.5514e-01, 1.3845e+01, 2.3734e+00, 3.2736e-02,\n        9.8053e-01, 7.9489e+00, 1.6534e+00, 7.0941e-01, 1.0999e-01, 4.0426e-01,\n        4.9481e+00, 8.6310e-02, 4.5669e-01, 1.2970e+01, 1.1514e+00, 8.4022e-01,\n        1.6246e+00, 2.7229e+00, 2.2242e+00, 2.6013e+00, 4.9501e-01, 3.9423e-01,\n        3.4091e-01, 3.4272e+00, 1.1160e+01, 3.9512e-01, 1.1832e+00, 1.2464e-01,\n        9.8178e-02, 1.6613e-01, 4.1290e-01, 6.1244e+00, 1.0231e+00, 1.2503e-01,\n        4.0771e-02, 8.0091e+00, 4.9205e+00, 1.7408e-01, 1.0122e+00, 5.9202e+00,\n        8.1931e-01, 9.6737e-01, 1.9622e+00, 4.5211e-01, 8.2471e+00, 3.1949e-01,\n        2.4146e-05, 1.1555e-01, 1.6288e+00, 9.0632e+00, 5.2839e+00, 2.3084e-01,\n        3.6019e-02, 8.4755e+00, 1.0514e-01, 4.7067e+00, 1.1707e-02, 8.6677e-01,\n        1.2755e-01, 2.2910e+00, 2.5207e+00, 1.3943e+00, 9.6922e+00, 6.4418e-03,\n        2.6435e+00, 1.5587e+00, 1.5745e+00, 6.7337e-02, 3.6581e-01, 1.9353e+00,\n        4.6241e-01, 7.6130e+00, 2.5071e-01, 1.2228e+00, 1.6051e+00, 4.3234e+00,\n        7.4199e-01, 2.6902e+00, 5.4231e+00, 1.2484e-01, 2.9254e+00, 8.4749e-01,\n        2.2955e+00, 1.5539e-02, 1.5911e-04, 2.7150e+00, 2.8486e-01, 6.2073e-01,\n        7.0398e+00, 7.0135e-01, 5.3422e+00, 3.0675e+00, 1.2243e+00, 2.5830e+00,\n        1.9938e+00, 1.3948e-02, 3.3332e+00, 5.7776e+00, 2.6297e-01, 7.4825e-01,\n        1.0251e-03, 1.0122e+00, 3.4252e-01, 2.3412e+00, 1.9059e+00, 8.1557e-02,\n        3.6914e-01, 1.4041e+01, 1.1974e-01, 1.0951e+00, 2.1961e-01, 6.2026e-02,\n        8.9509e-02, 3.2480e+00, 4.6860e-01, 1.5804e+00, 5.4547e-01, 9.0526e-01,\n        7.8704e-01, 4.9906e-01, 4.3199e-01, 8.9556e-01, 1.1273e+00, 8.8593e-01,\n        2.7353e+00, 2.3644e+00, 2.0056e-01, 1.2990e+00, 2.7394e+00, 4.0923e+00,\n        5.2077e-01, 5.5011e-02, 8.4863e-01, 3.8805e+00, 5.3181e-02, 1.4201e-01,\n        9.0159e-01, 2.0430e+00, 1.0488e+00, 3.0121e-01, 1.5229e-03, 5.8190e+00,\n        9.7950e+00, 2.2547e-01, 1.4066e+00, 6.5591e-01, 3.9774e+00, 2.9609e+00,\n        5.7000e-01, 4.4174e-01, 5.4463e-04, 1.4024e-01, 5.9563e+00, 5.1071e+00,\n        2.6194e-01, 1.2713e+01, 1.6933e+00, 3.5996e+00, 5.3956e+00, 3.8350e+00,\n        3.4070e+00, 3.7182e+00, 5.5391e-03, 4.3986e-01, 5.1933e-01, 1.2569e+00,\n        1.9752e+00, 1.3771e+00, 3.9889e-01, 4.6623e+00, 2.3725e+00, 3.6287e+00,\n        6.5457e-01, 1.1658e+00, 5.2813e-01, 1.5082e+00, 4.1374e+00, 2.2196e+00,\n        1.8648e+00, 7.9024e-02, 1.8012e-01, 6.4865e-01, 4.3497e-01, 1.9303e+00,\n        4.7323e-01, 3.0726e-02, 3.4314e+00, 1.1443e+00, 4.7556e+00, 5.9895e+00,\n        4.4451e-01, 1.2577e-01, 3.2947e+00, 6.4019e-01, 3.7777e+00, 2.6863e-02,\n        8.8693e-01, 5.8376e-02], device='cuda:0', dtype=torch.float64).mean())`"
     ]
    }
   ],
   "source": [
    "gt = task_red.get_summary()\n",
    "train_l, test_l  = get_dataloader(task_red,args)\n",
    "## Run SOI\n",
    "soi = SOI(args, nb_var = task_red.nb_var, gt = gt)\n",
    "## Fit the model\n",
    "soi.fit(train_l, test_l)\n",
    "## Compute O_information using the test_loader\n",
    "results = soi.compute_o_inf()\n",
    "print(\"SOI:\", {\" O-inf\": results[\"o_inf\"],\"tc\":results[\"tc\"], \"dtc\":results[\"dtc\"], \"S-info\": results[\"s_inf\"] })\n",
    "print(\"GT:\", {\" O-inf\": gt[\"o_inf\"],\"tc\":gt[\"tc\"], \"dtc\":gt[\"dtc\"], \"S-info\": gt[\"s_inf\"] } )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the synergy benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting the scores to learn \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | score     | DiT  | 623 K \n",
      "1 | model_ema | EMA  | 623 K \n",
      "-----------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.984     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                    | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85289ec9a8994c61ab84d3b8b3de8f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  35  GT:  -0.124 SOI_estimate:  -0.125\n",
      "Epoch:  40  GT:  -0.124 SOI_estimate:  -0.126\n",
      "Epoch:  45  GT:  -0.124 SOI_estimate:  -0.11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOI: {' O-inf': -0.14697401523590087, 'tc': 0.49834243655204774, 'dtc': 0.6453164517879486, 'S-info': 1.1436588883399963}\n",
      "GT: {' O-inf': -0.12391808195228782, 'tc': 0.5697171415941797, 'dtc': 0.6936352235464676, 'S-info': 1.2633523651406473}\n"
     ]
    }
   ],
   "source": [
    "gt = task_syn.get_summary()\n",
    "train_l, test_l  = get_dataloader(task_syn,args)\n",
    "## Run SOI\n",
    "soi = SOI(args, nb_var = task_syn.nb_var, gt = gt)\n",
    "## Fit the model\n",
    "soi.fit(train_l, test_l)\n",
    "## Compute O_information using the test_loader\n",
    "results = soi.compute_o_inf()\n",
    "print(\"SOI:\", {\" O-inf\": results[\"o_inf\"],\"tc\":results[\"tc\"], \"dtc\":results[\"dtc\"], \"S-info\": results[\"s_inf\"] })\n",
    "print(\"GT:\", {\" O-inf\": gt[\"o_inf\"],\"tc\":gt[\"tc\"], \"dtc\":gt[\"dtc\"], \"S-info\": gt[\"s_inf\"] } )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the mixed benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting the scores to learn \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | score     | DiT  | 624 K \n",
      "1 | model_ema | EMA  | 624 K \n",
      "-----------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.993     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                    | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dc40c18a144035861c637f7fa35b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  65  GT:  0.023 SOI_estimate:  0.056\n",
      "Epoch:  70  GT:  0.023 SOI_estimate:  0.03\n",
      "Epoch:  75  GT:  0.023 SOI_estimate:  0.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOI: {' O-inf': 0.04701898694038398, 'tc': 1.0358725666999817, 'dtc': 0.9888535797595978, 'S-info': 2.0247261464595794}\n",
      "GT: {' O-inf': 0.02345052029832928, 'tc': 1.0917791932861967, 'dtc': 1.0683286729878674, 'S-info': 2.160107866274064}\n"
     ]
    }
   ],
   "source": [
    "gt = task_both.get_summary()\n",
    "train_l, test_l  = get_dataloader(task_both,args)\n",
    "## Run SOI\n",
    "args.warmup_epochs = 60\n",
    "args.max_epochs = 80\n",
    "\n",
    "soi = SOI(args, nb_var = task_both.nb_var, gt = gt)\n",
    "## Fit the model\n",
    "soi.fit(train_l, test_l)\n",
    "## Compute O_information using the test_loader\n",
    "results = soi.compute_o_inf()\n",
    "print(\"SOI:\", {\" O-inf\": results[\"o_inf\"],\"tc\":results[\"tc\"], \"dtc\":results[\"dtc\"], \"S-info\": results[\"s_inf\"] })\n",
    "print(\"GT:\", {\" O-inf\": gt[\"o_inf\"],\"tc\":gt[\"tc\"], \"dtc\":gt[\"dtc\"], \"S-info\": gt[\"s_inf\"] } )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients of O-information estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_o_ing :  x0: 0.147, x1: 0.147, x2: 0.147, x3: -0.124, x4: -0.124, x5: -0.124,\n"
     ]
    }
   ],
   "source": [
    "gt = task_both.get_summary()\n",
    "print(\"g_o_ing : \",\" \".join([ \"x{}: {},\".format(i,np.round(x,decimals=3) ) for i,x in enumerate(gt[\"g_o_inf\"]) ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighting the scores to learn \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | score     | DiT  | 624 K \n",
      "1 | model_ema | EMA  | 624 K \n",
      "-----------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.993     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                    | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a72b8456444405084df12cc4db3cb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                           | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O_inf - GT:  0.023 O_inf - SOI_new:  0.044\n",
      "Gradient O_inf - GT:  x0: 0.147, x1: 0.147, x2: 0.147, x3: -0.124, x4: -0.124, x5: -0.124,\n",
      "Gradient O_inf - SOI: x0: 0.146, x1: 0.153, x2: 0.133, x3: -0.125, x4: -0.095, x5: -0.168,\n",
      "O_inf - GT:  0.023 O_inf - SOI_new:  0.041\n",
      "Gradient O_inf - GT:  x0: 0.147, x1: 0.147, x2: 0.147, x3: -0.124, x4: -0.124, x5: -0.124,\n",
      "Gradient O_inf - SOI: x0: 0.132, x1: 0.14, x2: 0.135, x3: -0.119, x4: -0.089, x5: -0.156,\n",
      "O_inf - GT:  0.023 O_inf - SOI_new:  0.018\n",
      "Gradient O_inf - GT:  x0: 0.147, x1: 0.147, x2: 0.147, x3: -0.124, x4: -0.124, x5: -0.124,\n",
      "Gradient O_inf - SOI: x0: 0.143, x1: 0.124, x2: 0.126, x3: -0.138, x4: -0.136, x5: -0.174,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                         | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tc': 1.01420716047287,\n",
       " 'dtc': 1.0057169377803803,\n",
       " 'o_inf': 0.008490222692489668,\n",
       " 's_inf': 2.0199240982532505,\n",
       " 'g_o_inf': {'x0': 0.13632701635360722,\n",
       "  'x1': 0.11200442314147951,\n",
       "  'x2': 0.11321437358856212,\n",
       "  'x3': -0.11913098394870753,\n",
       "  'x4': -0.1387510180473327,\n",
       "  'x5': -0.17350099682807918},\n",
       " 'tc_minus': {'x0': 0.6894628465175628,\n",
       "  'x1': 0.7175222396850586,\n",
       "  'x2': 0.7158963143825531,\n",
       "  'x3': 0.592874801158905,\n",
       "  'x4': 0.4880099058151245,\n",
       "  'x5': 0.8521396815776825},\n",
       " 'dtc_minus': {'x0': 0.8172996401786804,\n",
       "  'x1': 0.8210364401340484,\n",
       "  'x2': 0.8206204652786255,\n",
       "  'x3': 0.4652535945177078,\n",
       "  'x4': 0.3407686650753021,\n",
       "  'x5': 0.6701484620571136}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run SOI with gradient of o-information estimation\n",
    "args.warmup_epochs = 80\n",
    "args.max_epochs = 100\n",
    "## Run SOI\n",
    "gt = task_both.get_summary()\n",
    "train_l, test_l  = get_dataloader(task_both,args)\n",
    "soi = SOI_grad(args, nb_var = task_both.nb_var, gt = task_both.get_summary())\n",
    "## Fit the model\n",
    "soi.fit(train_l, test_l)\n",
    "## Compute O_information using the test_loader\n",
    "print(\"The final estimation :\", soi.compute_o_inf_with_grad() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
